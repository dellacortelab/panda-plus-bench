{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDA-PLUS-Bench: Evaluating WSI-Specific Feature Collapse\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dellacortelab/PANDA-PLUS-Bench/blob/main/PANDA_PLUS_Bench_Evaluation.ipynb)\n",
    "\n",
    "This notebook enables evaluation of pathology foundation models on the PANDA-PLUS-Bench benchmark for measuring WSI-specific feature collapse.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads pre-augmented patches from the benchmark dataset\n",
    "2. Extracts embeddings using your foundation model (or default Phikon)\n",
    "3. Computes robustness metrics (within-slide vs cross-slide accuracy, etc.)\n",
    "4. Compares your model against published results from 7 foundation models\n",
    "5. Generates publication-quality visualizations\n",
    "\n",
    "**Citation:**\n",
    "```\n",
    "Ebbert J, Della Corte D. PANDA-PLUS-Bench: A Benchmark for Evaluating \n",
    "WSI-Specific Feature Collapse in Pathology Foundation Models. 2025.\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Installation\n",
    "\n",
    "Run this cell first to install required packages (~2-3 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets huggingface_hub transformers timm torch torchvision\n",
    "!pip install -q scikit-learn scipy matplotlib seaborn tqdm pandas numpy\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"âš  No GPU detected. Embedding extraction will be slow.\")\n",
    "    print(\"  Go to Runtime â†’ Change runtime type â†’ GPU\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(\"\\nâœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "**Configure your evaluation here:**\n",
    "- Set your HuggingFace token (if using gated models like UNI, Virchow)\n",
    "- Choose which foundation model to evaluate\n",
    "- Select augmentation conditions to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration {display-mode: \"form\"}\n",
    "\n",
    "#@markdown ### HuggingFace Token (optional)\n",
    "#@markdown Required only for gated models (UNI, Virchow, etc.)\n",
    "#@markdown Get your token at: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Model Selection\n",
    "#@markdown Choose a model to evaluate, or enter a custom HuggingFace model ID\n",
    "MODEL_CHOICE = \"owkin/phikon\"  #@param [\"owkin/phikon\", \"owkin/phikon-v2\", \"MahmoodLab/UNI\", \"paige-ai/Virchow\", \"paige-ai/Virchow2\", \"custom\"]\n",
    "CUSTOM_MODEL_ID = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Augmentation Conditions\n",
    "#@markdown Select which augmentation conditions to evaluate\n",
    "EVAL_BASELINE = True  #@param {type:\"boolean\"}\n",
    "EVAL_COLOR_JITTER = False  #@param {type:\"boolean\"}\n",
    "EVAL_GRAYSCALE = False  #@param {type:\"boolean\"}\n",
    "EVAL_GAUSSIAN_NOISE = False  #@param {type:\"boolean\"}\n",
    "EVAL_HEAVY_GEOMETRIC = False  #@param {type:\"boolean\"}\n",
    "EVAL_COMBINED_AGGRESSIVE = True  #@param {type:\"boolean\"}\n",
    "EVAL_MACENKO = False  #@param {type:\"boolean\"}\n",
    "EVAL_HED = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Evaluation Settings\n",
    "N_NEIGHBORS = 5  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "\n",
    "# Process configuration\n",
    "MODEL_ID = CUSTOM_MODEL_ID if MODEL_CHOICE == \"custom\" else MODEL_CHOICE\n",
    "\n",
    "AUGMENTATIONS_TO_EVAL = []\n",
    "if EVAL_BASELINE: AUGMENTATIONS_TO_EVAL.append(\"baseline\")\n",
    "if EVAL_COLOR_JITTER: AUGMENTATIONS_TO_EVAL.append(\"color_jitter\")\n",
    "if EVAL_GRAYSCALE: AUGMENTATIONS_TO_EVAL.append(\"grayscale\")\n",
    "if EVAL_GAUSSIAN_NOISE: AUGMENTATIONS_TO_EVAL.append(\"gaussian_noise\")\n",
    "if EVAL_HEAVY_GEOMETRIC: AUGMENTATIONS_TO_EVAL.append(\"heavy_geometric\")\n",
    "if EVAL_COMBINED_AGGRESSIVE: AUGMENTATIONS_TO_EVAL.append(\"combined_aggressive\")\n",
    "if EVAL_MACENKO: AUGMENTATIONS_TO_EVAL.append(\"macenko_normalization\")\n",
    "if EVAL_HED: AUGMENTATIONS_TO_EVAL.append(\"hed_stain_augmentation\")\n",
    "\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Augmentations: {AUGMENTATIONS_TO_EVAL}\")\n",
    "print(f\"  HF Token: {'Set' if HF_TOKEN else 'Not set (using public models only)'}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Benchmark Dataset\n",
    "\n",
    "Downloads patches from HuggingFace Hub (~2-5 GB depending on augmentations selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Dataset repository\n",
    "DATASET_REPO = \"dellacorte/PANDA-PLUS-Bench\"\n",
    "\n",
    "print(\"Loading PANDA-PLUS-Bench dataset...\")\n",
    "print(\"(First run will download ~2-5 GB, subsequent runs use cache)\\n\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = load_dataset(DATASET_REPO, token=HF_TOKEN if HF_TOKEN else None)\n",
    "    print(f\"âœ“ Dataset loaded successfully!\")\n",
    "    print(f\"  Available splits: {list(dataset.keys())}\")\n",
    "    \n",
    "    # Show dataset info\n",
    "    sample_split = list(dataset.keys())[0]\n",
    "    print(f\"  Total patches in '{sample_split}': {len(dataset[sample_split])}\")\n",
    "    print(f\"  Features: {dataset[sample_split].features}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error loading dataset: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check that the dataset exists at the specified repo\")\n",
    "    print(\"  2. If private, ensure HF_TOKEN is set correctly\")\n",
    "    raise\n",
    "\n",
    "# Load paper results for comparison\n",
    "print(\"\\nLoading published benchmark results...\")\n",
    "try:\n",
    "    # Load from GitHub repo\n",
    "    import urllib.request\n",
    "    paper_results_url = \"https://raw.githubusercontent.com/dellacortelab/PANDA-PLUS-Bench/main/paper_results.json\"\n",
    "    with urllib.request.urlopen(paper_results_url) as response:\n",
    "        PAPER_RESULTS = json.loads(response.read().decode())\n",
    "    print(f\"âœ“ Loaded results for {len(PAPER_RESULTS)} models from paper\")\n",
    "    print(f\"  Models: {list(PAPER_RESULTS.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Paper results not found: {e}\")\n",
    "    print(\"  Comparison will be skipped.\")\n",
    "    PAPER_RESULTS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Foundation Model\n",
    "\n",
    "Loads the selected foundation model for embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_foundation_model(model_id, hf_token=None):\n",
    "    \"\"\"\n",
    "    Load a pathology foundation model.\n",
    "    \n",
    "    Supports:\n",
    "    - Phikon / Phikon-v2 (owkin)\n",
    "    - UNI / UNI2 (MahmoodLab)\n",
    "    - Virchow / Virchow2 (paige-ai)\n",
    "    - Any timm-compatible model\n",
    "    - Any HuggingFace transformers model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    \n",
    "    model = None\n",
    "    processor = None\n",
    "    embed_dim = None\n",
    "    \n",
    "    # Try HuggingFace transformers first\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_id, \n",
    "            token=hf_token,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            processor = AutoImageProcessor.from_pretrained(\n",
    "                model_id, \n",
    "                token=hf_token\n",
    "            )\n",
    "        except:\n",
    "            processor = None\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        if hasattr(model.config, 'hidden_size'):\n",
    "            embed_dim = model.config.hidden_size\n",
    "        elif hasattr(model, 'embed_dim'):\n",
    "            embed_dim = model.embed_dim\n",
    "        else:\n",
    "            embed_dim = None\n",
    "            \n",
    "        print(f\"  âœ“ Loaded via HuggingFace transformers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  HuggingFace loading failed: {e}\")\n",
    "        print(f\"  Trying timm...\")\n",
    "        \n",
    "        # Try timm\n",
    "        try:\n",
    "            model = timm.create_model(model_id, pretrained=True, num_classes=0)\n",
    "            embed_dim = model.num_features\n",
    "            processor = None\n",
    "            print(f\"  âœ“ Loaded via timm\")\n",
    "        except Exception as e2:\n",
    "            raise ValueError(f\"Could not load model '{model_id}' via HuggingFace or timm: {e2}\")\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Default transform if no processor\n",
    "    if processor is None:\n",
    "        default_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        default_transform = None\n",
    "    \n",
    "    print(f\"  Embedding dimension: {embed_dim}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    \n",
    "    return model, processor, default_transform, embed_dim\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model, processor, transform, embed_dim = load_foundation_model(MODEL_ID, HF_TOKEN)\n",
    "print(\"\\nâœ“ Model ready for embedding extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Embeddings\n",
    "\n",
    "Extracts embeddings for all patches in selected augmentation conditions.\n",
    "\n",
    "**Expected time:** ~5-15 minutes per augmentation on Colab GPU (T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for benchmark patches.\"\"\"\n",
    "    def __init__(self, hf_dataset, processor=None, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Get image (handle different formats)\n",
    "        if 'image' in item:\n",
    "            image = item['image']\n",
    "        elif 'patch' in item:\n",
    "            image = item['patch']\n",
    "        else:\n",
    "            raise KeyError(f\"No image field found. Available: {item.keys()}\")\n",
    "        \n",
    "        # Convert to PIL if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        elif self.transform is not None:\n",
    "            pixel_values = self.transform(image)\n",
    "        else:\n",
    "            raise ValueError(\"No processor or transform provided\")\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'label': item.get('label', -1),\n",
    "            'slide_id': item.get('slide_id', 'unknown')\n",
    "        }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extract embeddings from all patches.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (n_patches, embed_dim)\n",
    "        labels: numpy array of shape (n_patches,)\n",
    "        slide_ids: numpy array of shape (n_patches,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_slide_ids = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "        # Extract embeddings (handle different output formats)\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        elif hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            embeddings = outputs.pooler_output\n",
    "        elif isinstance(outputs, torch.Tensor):\n",
    "            embeddings = outputs\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output format: {type(outputs)}\")\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_labels.extend(batch['label'].numpy())\n",
    "        all_slide_ids.extend(batch['slide_id'])\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    labels = np.array(all_labels)\n",
    "    slide_ids = np.array(all_slide_ids)\n",
    "    \n",
    "    return embeddings, labels, slide_ids\n",
    "\n",
    "\n",
    "# Extract embeddings for each augmentation\n",
    "print(f\"\\nExtracting embeddings for {len(AUGMENTATIONS_TO_EVAL)} augmentation(s)...\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for aug in AUGMENTATIONS_TO_EVAL:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {aug}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if aug not in dataset:\n",
    "        print(f\"  âš  Augmentation '{aug}' not found in dataset. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    patch_dataset = PatchDataset(\n",
    "        dataset[aug], \n",
    "        processor=processor, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        patch_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  Patches: {len(patch_dataset)}\")\n",
    "    print(f\"  Batches: {len(dataloader)}\")\n",
    "    \n",
    "    embeddings, labels, slide_ids = extract_embeddings(model, dataloader, DEVICE)\n",
    "    \n",
    "    all_results[aug] = {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': labels,\n",
    "        'slide_ids': slide_ids\n",
    "    }\n",
    "    \n",
    "    print(f\"  âœ“ Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"  âœ“ Unique slides: {len(np.unique(slide_ids))}\")\n",
    "    print(f\"  âœ“ Unique labels: {np.unique(labels)}\")\n",
    "\n",
    "print(f\"\\n\\nâœ“ Embedding extraction complete!\")\n",
    "print(f\"  Processed {len(all_results)} augmentation(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Benchmark Metrics\n",
    "\n",
    "Computes the standard PANDA-PLUS-Bench metrics:\n",
    "- **Within-slide accuracy**: Classification using patches from same slide\n",
    "- **Cross-slide accuracy**: Leave-one-slide-out classification\n",
    "- **Accuracy gap**: Within - Cross (higher = more collapse)\n",
    "- **Silhouette scores**: Cluster quality by class and by slide\n",
    "- **Confusion attribution**: Entropy of kNN label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_within_cross_accuracy(embeddings, labels, slide_ids, n_neighbors=5):\n",
    "    \"\"\"Compute within-slide and cross-slide classification accuracy.\"\"\"\n",
    "    unique_slides = np.unique(slide_ids)\n",
    "    within_scores = []\n",
    "    cross_scores = []\n",
    "    \n",
    "    # Within-slide accuracy\n",
    "    for slide in unique_slides:\n",
    "        slide_mask = slide_ids == slide\n",
    "        slide_embeddings = embeddings[slide_mask]\n",
    "        slide_labels = labels[slide_mask]\n",
    "        \n",
    "        if len(slide_embeddings) < n_neighbors + 1:\n",
    "            continue\n",
    "        \n",
    "        n_test = max(1, len(slide_embeddings) // 5)\n",
    "        indices = np.random.permutation(len(slide_embeddings))\n",
    "        train_idx, test_idx = indices[:-n_test], indices[-n_test:]\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=min(n_neighbors, len(train_idx)-1))\n",
    "        knn.fit(slide_embeddings[train_idx], slide_labels[train_idx])\n",
    "        within_scores.append(knn.score(slide_embeddings[test_idx], slide_labels[test_idx]))\n",
    "    \n",
    "    # Cross-slide accuracy\n",
    "    logo = LeaveOneGroupOut()\n",
    "    for train_idx, test_idx in logo.split(embeddings, labels, slide_ids):\n",
    "        X_train, X_test = embeddings[train_idx], embeddings[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, y_train)\n",
    "        cross_scores.append(knn.score(X_test, y_test))\n",
    "    \n",
    "    return np.mean(within_scores), np.mean(cross_scores)\n",
    "\n",
    "# Compute metrics for each augmentation\n",
    "print(\"\\nComputing metrics...\\n\")\n",
    "metrics = {}\n",
    "\n",
    "for aug, results in all_results.items():\n",
    "    print(f\"Computing metrics for {aug}...\")\n",
    "    \n",
    "    within_acc, cross_acc = compute_within_cross_accuracy(\n",
    "        results['embeddings'],\n",
    "        results['labels'],\n",
    "        results['slide_ids'],\n",
    "        n_neighbors=N_NEIGHBORS\n",
    "    )\n",
    "    \n",
    "    metrics[aug] = {\n",
    "        'within_accuracy': within_acc,\n",
    "        'cross_accuracy': cross_acc,\n",
    "        'accuracy_gap': within_acc - cross_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Within-slide accuracy: {within_acc:.3f}\")\n",
    "    print(f\"  Cross-slide accuracy: {cross_acc:.3f}\")\n",
    "    print(f\"  Accuracy gap: {within_acc - cross_acc:.3f}\\n\")\n",
    "\n",
    "print(\"âœ“ Metrics computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Results\n",
    "\n",
    "Display your model's results in a summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Create results dataframe\n",
    "results_data = []\n",
    "for aug, m in metrics.items():\n",
    "    results_data.append({\n",
    "        'Augmentation': aug,\n",
    "        'Within-Slide Acc': m['within_accuracy'],\n",
    "        'Cross-Slide Acc': m['cross_accuracy'],\n",
    "        'Accuracy Gap': m['accuracy_gap']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Display table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Results for {MODEL_ID}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "display_df = results_df.copy()\n",
    "display_df['Within-Slide Acc'] = display_df['Within-Slide Acc'].apply(lambda x: f\"{x:.3f}\")\n",
    "display_df['Cross-Slide Acc'] = display_df['Cross-Slide Acc'].apply(lambda x: f\"{x:.3f}\")\n",
    "display_df['Accuracy Gap'] = display_df['Accuracy Gap'].apply(lambda x: f\"{x:.3f}\")\n",
    "print(display_df.to_string(index=False))\n",
    "print(f\"\\n{'='*70}\")\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Within vs Cross-slide accuracy\n",
    "ax1 = axes[0]\n",
    "x = range(len(results_df))\n",
    "width = 0.35\n",
    "ax1.bar([i - width/2 for i in x], results_df['Within-Slide Acc'], width, \n",
    "        label='Within-Slide', alpha=0.8, color='#2ecc71')\n",
    "ax1.bar([i + width/2 for i in x], results_df['Cross-Slide Acc'], width,\n",
    "        label='Cross-Slide', alpha=0.8, color='#3498db')\n",
    "ax1.set_xlabel('Augmentation', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "ax1.set_title('Within-Slide vs Cross-Slide Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(results_df['Augmentation'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy gap\n",
    "ax2 = axes[1]\n",
    "colors = ['#e74c3c' if gap > 0.2 else '#f39c12' if gap > 0.1 else '#2ecc71' \n",
    "          for gap in results_df['Accuracy Gap']]\n",
    "ax2.bar(x, results_df['Accuracy Gap'], alpha=0.8, color=colors)\n",
    "ax2.axhline(y=0.2, color='red', linestyle='--', linewidth=1, alpha=0.5, label='High collapse (>0.20)')\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Moderate collapse (>0.10)')\n",
    "ax2.set_xlabel('Augmentation', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy Gap', fontsize=12)\n",
    "ax2.set_title('Feature Collapse (Accuracy Gap)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(results_df['Augmentation'], rotation=45, ha='right')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Summary Statistics\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Average Within-Slide Accuracy: {results_df['Within-Slide Acc'].mean():.3f}\")\n",
    "print(f\"Average Cross-Slide Accuracy:  {results_df['Cross-Slide Acc'].mean():.3f}\")\n",
    "print(f\"Average Accuracy Gap:          {results_df['Accuracy Gap'].mean():.3f}\")\n",
    "print(f\"Max Accuracy Gap:              {results_df['Accuracy Gap'].max():.3f}\")\n",
    "print(f\"Min Accuracy Gap:              {results_df['Accuracy Gap'].min():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Published Results\n",
    "\n",
    "Compare your model's performance against published foundation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PAPER_RESULTS:\n",
    "    # Prepare comparison data\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Add your model results\n",
    "    for aug in AUGMENTATIONS_TO_EVAL:\n",
    "        if aug in metrics:\n",
    "            comparison_data.append({\n",
    "                'Model': MODEL_ID.split('/')[-1],\n",
    "                'Augmentation': aug,\n",
    "                'Within-Slide Acc': metrics[aug]['within_accuracy'],\n",
    "                'Cross-Slide Acc': metrics[aug]['cross_accuracy'],\n",
    "                'Accuracy Gap': metrics[aug]['accuracy_gap']\n",
    "            })\n",
    "    \n",
    "    # Add paper results\n",
    "    for model_name, model_results in PAPER_RESULTS.items():\n",
    "        for aug in AUGMENTATIONS_TO_EVAL:\n",
    "            if aug in model_results:\n",
    "                comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Augmentation': aug,\n",
    "                    'Within-Slide Acc': model_results[aug]['within_slide_accuracy'],\n",
    "                    'Cross-Slide Acc': model_results[aug]['cross_slide_accuracy'],\n",
    "                    'Accuracy Gap': model_results[aug]['accuracy_gap']\n",
    "                })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot comparison for each augmentation\n",
    "    for aug in AUGMENTATIONS_TO_EVAL:\n",
    "        if aug not in comparison_df['Augmentation'].values:\n",
    "            continue\n",
    "            \n",
    "        aug_data = comparison_df[comparison_df['Augmentation'] == aug].sort_values('Accuracy Gap')\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        x = range(len(aug_data))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar([i - width/2 for i in x], aug_data['Within-Slide Acc'], width,\n",
    "               label='Within-Slide', alpha=0.8, color='#2ecc71')\n",
    "        ax.bar([i + width/2 for i in x], aug_data['Cross-Slide Acc'], width,\n",
    "               label='Cross-Slide', alpha=0.8, color='#3498db')\n",
    "        \n",
    "        # Highlight your model\n",
    "        your_model_idx = aug_data[aug_data['Model'] == MODEL_ID.split('/')[-1]].index\n",
    "        if len(your_model_idx) > 0:\n",
    "            idx_pos = list(aug_data.index).index(your_model_idx[0])\n",
    "            ax.axvspan(idx_pos - 0.5, idx_pos + 0.5, alpha=0.2, color='yellow', zorder=0)\n",
    "        \n",
    "        ax.set_xlabel('Model', fontsize=12)\n",
    "        ax.set_ylabel('Accuracy', fontsize=12)\n",
    "        ax.set_title(f'Model Comparison - {aug.replace(\"_\", \" \").title()}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(aug_data['Model'], rotation=45, ha='right')\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print ranking\n",
    "        print(f\"\\n{aug.replace('_', ' ').title()} - Accuracy Gap Ranking (lower is better):\")\n",
    "        print(\"=\"*70)\n",
    "        for i, (idx, row) in enumerate(aug_data.iterrows(), 1):\n",
    "            marker = \" â† YOUR MODEL\" if row['Model'] == MODEL_ID.split('/')[-1] else \"\"\n",
    "            print(f\"{i}. {row['Model']:20s} | Gap: {row['Accuracy Gap']:.3f} | \"\n",
    "                  f\"Within: {row['Within-Slide Acc']:.3f} | Cross: {row['Cross-Slide Acc']:.3f}{marker}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "else:\n",
    "    print(\"âš  No paper results available for comparison.\")\n",
    "    print(\"  Your model's results are displayed above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Embedding Space Analysis\n",
    "\n",
    "Visualize and analyze the structure of the embedding space to understand feature collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Select augmentation to visualize (prefer baseline if available)\n",
    "viz_aug = 'baseline' if 'baseline' in all_results else list(all_results.keys())[0]\n",
    "viz_data = all_results[viz_aug]\n",
    "\n",
    "print(f\"Visualizing embedding space for: {viz_aug}\")\n",
    "print(f\"Total samples: {len(viz_data['embeddings'])}\")\n",
    "\n",
    "# Subsample for faster visualization if needed\n",
    "max_samples = 2000\n",
    "if len(viz_data['embeddings']) > max_samples:\n",
    "    print(f\"Subsampling to {max_samples} patches for visualization...\")\n",
    "    indices = np.random.choice(len(viz_data['embeddings']), max_samples, replace=False)\n",
    "    viz_embeddings = viz_data['embeddings'][indices]\n",
    "    viz_labels = viz_data['labels'][indices]\n",
    "    viz_slide_ids = viz_data['slide_ids'][indices]\n",
    "else:\n",
    "    viz_embeddings = viz_data['embeddings']\n",
    "    viz_labels = viz_data['labels']\n",
    "    viz_slide_ids = viz_data['slide_ids']\n",
    "\n",
    "# Apply PCA first for dimensionality reduction\n",
    "print(\"Applying PCA...\")\n",
    "pca = PCA(n_components=50)\n",
    "embeddings_pca = pca.fit_transform(viz_embeddings)\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_[:10].sum():.2%} (first 10 components)\")\n",
    "\n",
    "# Apply t-SNE\n",
    "print(\"Applying t-SNE (this may take a few minutes)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Color by class label\n",
    "ax1 = axes[0]\n",
    "scatter1 = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                       c=viz_labels, cmap='tab10', alpha=0.6, s=20)\n",
    "ax1.set_title('Embedding Space - Colored by ISUP Grade', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "ax1.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1)\n",
    "cbar1.set_label('ISUP Grade', fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Color by slide ID\n",
    "ax2 = axes[1]\n",
    "unique_slides = np.unique(viz_slide_ids)\n",
    "slide_to_int = {slide: i for i, slide in enumerate(unique_slides)}\n",
    "slide_colors = np.array([slide_to_int[s] for s in viz_slide_ids])\n",
    "scatter2 = ax2.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],\n",
    "                       c=slide_colors, cmap='tab20', alpha=0.6, s=20)\n",
    "ax2.set_title('Embedding Space - Colored by Slide ID', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "ax2.set_ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Interpretation Guide:\")\n",
    "print(\"=\"*70)\n",
    "print(\"LEFT PLOT (colored by grade):\")\n",
    "print(\"  â€¢ Good: Clear separation between different grades\")\n",
    "print(\"  â€¢ Poor: Mixed colors throughout (grade confusion)\")\n",
    "print(\"\\nRIGHT PLOT (colored by slide):\")\n",
    "print(\"  â€¢ Good: Mixed colors throughout (slide-independent features)\")\n",
    "print(\"  â€¢ Poor: Distinct clusters by color (slide-specific features)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Advanced Metrics and Analysis\n",
    "\n",
    "Compute additional metrics including silhouette scores, k-NN analysis, and robustness measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def compute_advanced_metrics(embeddings, labels, slide_ids, k=50):\n",
    "    \"\"\"Compute additional diagnostic metrics.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Subsample for efficiency\n",
    "    max_samples = 5000\n",
    "    if len(embeddings) > max_samples:\n",
    "        indices = np.random.choice(len(embeddings), max_samples, replace=False)\n",
    "        embeddings = embeddings[indices]\n",
    "        labels = labels[indices]\n",
    "        slide_ids = slide_ids[indices]\n",
    "    \n",
    "    # Silhouette scores\n",
    "    try:\n",
    "        metrics['silhouette_class'] = silhouette_score(embeddings, labels)\n",
    "    except:\n",
    "        metrics['silhouette_class'] = 0.0\n",
    "    \n",
    "    try:\n",
    "        metrics['silhouette_slide'] = silhouette_score(embeddings, slide_ids)\n",
    "    except:\n",
    "        metrics['silhouette_slide'] = 0.0\n",
    "    \n",
    "    # k-NN same-slide fraction\n",
    "    nn = NearestNeighbors(n_neighbors=min(k, len(embeddings)-1))\n",
    "    nn.fit(embeddings)\n",
    "    \n",
    "    same_slide_counts = []\n",
    "    for i in range(len(embeddings)):\n",
    "        distances, indices = nn.kneighbors([embeddings[i]])\n",
    "        neighbor_slides = slide_ids[indices[0][1:]]  # Exclude self\n",
    "        same_slide = (neighbor_slides == slide_ids[i]).sum()\n",
    "        same_slide_counts.append(same_slide)\n",
    "    \n",
    "    metrics['knn_same_slide_fraction'] = np.mean(same_slide_counts) / (k - 1)\n",
    "    \n",
    "    # Slide ID prediction accuracy (how well can we predict slide from embeddings)\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    try:\n",
    "        # Encode slide IDs as integers\n",
    "        unique_slides = np.unique(slide_ids)\n",
    "        slide_to_int = {slide: i for i, slide in enumerate(unique_slides)}\n",
    "        slide_ints = np.array([slide_to_int[s] for s in slide_ids])\n",
    "        \n",
    "        clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        scores = cross_val_score(clf, embeddings, slide_ints, cv=5)\n",
    "        metrics['slide_id_accuracy'] = scores.mean()\n",
    "    except:\n",
    "        metrics['slide_id_accuracy'] = 0.0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Compute advanced metrics for each augmentation\n",
    "print(\"Computing advanced metrics...\\n\")\n",
    "advanced_metrics = {}\n",
    "\n",
    "for aug, results in all_results.items():\n",
    "    print(f\"Processing {aug}...\")\n",
    "    adv_m = compute_advanced_metrics(\n",
    "        results['embeddings'],\n",
    "        results['labels'],\n",
    "        results['slide_ids']\n",
    "    )\n",
    "    advanced_metrics[aug] = adv_m\n",
    "    \n",
    "    print(f\"  Silhouette (class): {adv_m['silhouette_class']:7.3f} (higher = better class separation)\")\n",
    "    print(f\"  Silhouette (slide): {adv_m['silhouette_slide']:7.3f} (lower = less slide clustering)\")\n",
    "    print(f\"  k-NN same slide:    {adv_m['knn_same_slide_fraction']:7.3f} (lower = less slide bias)\")\n",
    "    print(f\"  Slide ID predict:   {adv_m['slide_id_accuracy']:7.3f} (lower = less slide information)\")\n",
    "    print()\n",
    "\n",
    "# Create comprehensive metrics table\n",
    "comprehensive_data = []\n",
    "for aug in all_results.keys():\n",
    "    row = {\n",
    "        'Augmentation': aug,\n",
    "        'Within Acc': metrics[aug]['within_accuracy'],\n",
    "        'Cross Acc': metrics[aug]['cross_accuracy'],\n",
    "        'Gap': metrics[aug]['accuracy_gap'],\n",
    "        'Sil-Class': advanced_metrics[aug]['silhouette_class'],\n",
    "        'Sil-Slide': advanced_metrics[aug]['silhouette_slide'],\n",
    "        'k-NN-Slide': advanced_metrics[aug]['knn_same_slide_fraction'],\n",
    "        'Slide-ID': advanced_metrics[aug]['slide_id_accuracy']\n",
    "    }\n",
    "    comprehensive_data.append(row)\n",
    "\n",
    "comp_df = pd.DataFrame(comprehensive_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Comprehensive Metrics Summary\")\n",
    "print(\"=\"*90)\n",
    "print(comp_df.to_string(index=False, float_format='%.3f'))\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Visualize advanced metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "aug_names = [aug.replace('_', ' ').title() for aug in comp_df['Augmentation']]\n",
    "x = range(len(comp_df))\n",
    "\n",
    "# Silhouette scores\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar([i - 0.2 for i in x], comp_df['Sil-Class'], 0.4, label='Class', alpha=0.8, color='#2ecc71')\n",
    "ax1.bar([i + 0.2 for i in x], comp_df['Sil-Slide'], 0.4, label='Slide', alpha=0.8, color='#e74c3c')\n",
    "ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax1.set_title('Silhouette Scores', fontweight='bold')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(aug_names, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# k-NN same slide fraction\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['#e74c3c' if v > 0.6 else '#f39c12' if v > 0.5 else '#2ecc71' \n",
    "          for v in comp_df['k-NN-Slide']]\n",
    "ax2.bar(x, comp_df['k-NN-Slide'], alpha=0.8, color=colors)\n",
    "ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='High bias (>0.50)')\n",
    "ax2.set_title('k-NN Same-Slide Fraction', fontweight='bold')\n",
    "ax2.set_ylabel('Fraction')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(aug_names, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Slide ID prediction accuracy\n",
    "ax3 = axes[1, 0]\n",
    "colors = ['#e74c3c' if v > 0.8 else '#f39c12' if v > 0.7 else '#2ecc71' \n",
    "          for v in comp_df['Slide-ID']]\n",
    "ax3.bar(x, comp_df['Slide-ID'], alpha=0.8, color=colors)\n",
    "ax3.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='High leakage (>0.80)')\n",
    "ax3.set_title('Slide ID Prediction Accuracy', fontweight='bold')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(aug_names, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Accuracy gap (repeated for comparison)\n",
    "ax4 = axes[1, 1]\n",
    "colors = ['#e74c3c' if v > 0.2 else '#f39c12' if v > 0.1 else '#2ecc71' \n",
    "          for v in comp_df['Gap']]\n",
    "ax4.bar(x, comp_df['Gap'], alpha=0.8, color=colors)\n",
    "ax4.axhline(y=0.2, color='red', linestyle='--', alpha=0.5)\n",
    "ax4.set_title('Accuracy Gap', fontweight='bold')\n",
    "ax4.set_ylabel('Gap')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(aug_names, rotation=45, ha='right')\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"Metric Interpretation:\")\n",
    "print(\"=\"*90)\n",
    "print(\"Silhouette (class):  Higher is better - indicates good class separation\")\n",
    "print(\"Silhouette (slide):  Lower is better - indicates less slide-specific clustering\")\n",
    "print(\"k-NN same slide:     Lower is better - fewer neighbors from same slide\")\n",
    "print(\"Slide ID predict:    Lower is better - harder to identify slide from embeddings\")\n",
    "print(\"Accuracy gap:        Lower is better - less overfitting to slide identity\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Summary and Recommendations\n",
    "\n",
    "Final assessment of your model's performance on the PANDA-PLUS-Bench benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model_performance(metrics_dict, advanced_metrics_dict):\n",
    "    \"\"\"Generate assessment and recommendations.\"\"\"\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_gap = np.mean([m['accuracy_gap'] for m in metrics_dict.values()])\n",
    "    avg_cross_acc = np.mean([m['cross_accuracy'] for m in metrics_dict.values()])\n",
    "    avg_knn_slide = np.mean([m['knn_same_slide_fraction'] for m in advanced_metrics_dict.values()])\n",
    "    avg_slide_id = np.mean([m['slide_id_accuracy'] for m in advanced_metrics_dict.values()])\n",
    "    \n",
    "    # Scoring\n",
    "    scores = {\n",
    "        'Feature Collapse': 'Excellent' if avg_gap < 0.15 else 'Good' if avg_gap < 0.20 else 'Moderate' if avg_gap < 0.25 else 'Poor',\n",
    "        'Cross-Slide Performance': 'Excellent' if avg_cross_acc > 0.55 else 'Good' if avg_cross_acc > 0.50 else 'Moderate' if avg_cross_acc > 0.45 else 'Poor',\n",
    "        'Slide Independence': 'Excellent' if avg_knn_slide < 0.50 else 'Good' if avg_knn_slide < 0.55 else 'Moderate' if avg_knn_slide < 0.60 else 'Poor',\n",
    "        'Information Leakage': 'Excellent' if avg_slide_id < 0.75 else 'Good' if avg_slide_id < 0.80 else 'Moderate' if avg_slide_id < 0.85 else 'Poor'\n",
    "    }\n",
    "    \n",
    "    return scores, avg_gap, avg_cross_acc, avg_knn_slide, avg_slide_id\n",
    "\n",
    "# Generate assessment\n",
    "scores, avg_gap, avg_cross_acc, avg_knn_slide, avg_slide_id = assess_model_performance(\n",
    "    metrics, advanced_metrics\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"PANDA-PLUS-Bench Assessment: {MODEL_ID}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Overall score\n",
    "overall_ratings = list(scores.values())\n",
    "if overall_ratings.count('Excellent') >= 3:\n",
    "    overall = \"ðŸŒŸ EXCELLENT\"\n",
    "    color = '\\033[92m'  # Green\n",
    "elif overall_ratings.count('Poor') == 0:\n",
    "    overall = \"âœ“ GOOD\"\n",
    "    color = '\\033[93m'  # Yellow\n",
    "else:\n",
    "    overall = \"âš  NEEDS IMPROVEMENT\"\n",
    "    color = '\\033[91m'  # Red\n",
    "\n",
    "print(f\"Overall Rating: {color}{overall}\\033[0m\\n\")\n",
    "\n",
    "# Detailed scores\n",
    "print(\"Detailed Assessment:\")\n",
    "print(\"-\" * 70)\n",
    "for metric, rating in scores.items():\n",
    "    emoji = \"ðŸŒŸ\" if rating == \"Excellent\" else \"âœ“\" if rating == \"Good\" else \"â—‹\" if rating == \"Moderate\" else \"âœ—\"\n",
    "    print(f\"{emoji} {metric:25s}: {rating}\")\n",
    "print()\n",
    "\n",
    "# Key metrics\n",
    "print(\"Key Metrics:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Average Accuracy Gap:          {avg_gap:.3f}\")\n",
    "print(f\"  Average Cross-Slide Accuracy:  {avg_cross_acc:.3f}\")\n",
    "print(f\"  Average k-NN Same-Slide:       {avg_knn_slide:.3f}\")\n",
    "print(f\"  Average Slide ID Prediction:   {avg_slide_id:.3f}\")\n",
    "print()\n",
    "\n",
    "# Recommendations\n",
    "print(\"Recommendations:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if avg_gap > 0.20:\n",
    "    print(\"âš  HIGH FEATURE COLLAPSE DETECTED\")\n",
    "    print(\"  â†’ Consider additional regularization during training\")\n",
    "    print(\"  â†’ Implement slide-level data augmentation\")\n",
    "    print(\"  â†’ Use contrastive learning with slide-aware negative sampling\")\n",
    "    \n",
    "if avg_cross_acc < 0.50:\n",
    "    print(\"âš  LOW CROSS-SLIDE GENERALIZATION\")\n",
    "    print(\"  â†’ Train on more diverse datasets\")\n",
    "    print(\"  â†’ Implement domain adaptation techniques\")\n",
    "    print(\"  â†’ Consider stain normalization preprocessing\")\n",
    "    \n",
    "if avg_knn_slide > 0.60:\n",
    "    print(\"âš  HIGH SLIDE-SPECIFIC CLUSTERING\")\n",
    "    print(\"  â†’ Embeddings are capturing slide-specific artifacts\")\n",
    "    print(\"  â†’ Review preprocessing pipeline for scanner/stain bias\")\n",
    "    print(\"  â†’ Consider adversarial training to remove slide information\")\n",
    "    \n",
    "if avg_slide_id > 0.85:\n",
    "    print(\"âš  SEVERE INFORMATION LEAKAGE\")\n",
    "    print(\"  â†’ Model is encoding slide identity in embeddings\")\n",
    "    print(\"  â†’ Critical issue for downstream clinical applications\")\n",
    "    print(\"  â†’ Requires model retraining with bias mitigation\")\n",
    "\n",
    "if avg_gap < 0.15 and avg_cross_acc > 0.50:\n",
    "    print(\"âœ“ EXCELLENT PERFORMANCE\")\n",
    "    print(\"  â†’ Model shows strong generalization\")\n",
    "    print(\"  â†’ Low feature collapse indicates robust representations\")\n",
    "    print(\"  â†’ Suitable for multi-site clinical deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Test on additional augmentation conditions for robustness\")\n",
    "print(\"2. Evaluate on external validation cohorts\")\n",
    "print(\"3. Compare with other foundation models using the rankings above\")\n",
    "print(\"4. Consider fine-tuning on downstream tasks with appropriate safeguards\")\n",
    "print(\"5. Share your results with the community!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Create a final summary visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "categories = list(scores.keys())\n",
    "ratings_numeric = {\n",
    "    'Excellent': 4,\n",
    "    'Good': 3,\n",
    "    'Moderate': 2,\n",
    "    'Poor': 1\n",
    "}\n",
    "values = [ratings_numeric[scores[cat]] for cat in categories]\n",
    "colors_map = {4: '#2ecc71', 3: '#3498db', 2: '#f39c12', 1: '#e74c3c'}\n",
    "bar_colors = [colors_map[v] for v in values]\n",
    "\n",
    "y_pos = range(len(categories))\n",
    "bars = ax.barh(y_pos, values, color=bar_colors, alpha=0.8)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(categories)\n",
    "ax.set_xlabel('Rating Score', fontsize=12)\n",
    "ax.set_title(f'PANDA-PLUS-Bench Performance Summary\\n{MODEL_ID}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 4.5)\n",
    "ax.set_xticks([1, 2, 3, 4])\n",
    "ax.set_xticklabels(['Poor', 'Moderate', 'Good', 'Excellent'])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val, rating) in enumerate(zip(bars, values, scores.values())):\n",
    "    ax.text(val + 0.1, i, rating, va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
