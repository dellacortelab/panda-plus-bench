{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDA-PLUS-Bench: Evaluating WSI-Specific Feature Collapse\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dellacortelab/PANDA-PLUS-Bench/blob/main/PANDA_PLUS_Bench_Evaluation.ipynb)\n",
    "\n",
    "This notebook enables evaluation of pathology foundation models on the PANDA-PLUS-Bench benchmark for measuring WSI-specific feature collapse.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Loads pre-augmented patches from the benchmark dataset\n",
    "2. Extracts embeddings using your foundation model (or default Phikon)\n",
    "3. Computes robustness metrics (within-slide vs cross-slide accuracy, etc.)\n",
    "4. Compares your model against published results from 7 foundation models\n",
    "5. Generates publication-quality visualizations\n",
    "\n",
    "**Citation:**\n",
    "```\n",
    "Ebbert J, Della Corte D. PANDA-PLUS-Bench: A Benchmark for Evaluating \n",
    "WSI-Specific Feature Collapse in Pathology Foundation Models. 2025.\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Installation\n",
    "\n",
    "Run this cell first to install required packages (~2-3 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q datasets huggingface_hub transformers timm torch torchvision\n",
    "!pip install -q scikit-learn scipy matplotlib seaborn tqdm pandas numpy\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verify GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    print(\"⚠ No GPU detected. Embedding extraction will be slow.\")\n",
    "    print(\"  Go to Runtime → Change runtime type → GPU\")\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(\"\\n✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "**Configure your evaluation here:**\n",
    "- Set your HuggingFace token (if using gated models like UNI, Virchow)\n",
    "- Choose which foundation model to evaluate\n",
    "- Select augmentation conditions to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration {display-mode: \"form\"}\n",
    "\n",
    "#@markdown ### HuggingFace Token (optional)\n",
    "#@markdown Required only for gated models (UNI, Virchow, etc.)\n",
    "#@markdown Get your token at: https://huggingface.co/settings/tokens\n",
    "HF_TOKEN = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Model Selection\n",
    "#@markdown Choose a model to evaluate, or enter a custom HuggingFace model ID\n",
    "MODEL_CHOICE = \"owkin/phikon\"  #@param [\"owkin/phikon\", \"owkin/phikon-v2\", \"MahmoodLab/UNI\", \"paige-ai/Virchow\", \"paige-ai/Virchow2\", \"custom\"]\n",
    "CUSTOM_MODEL_ID = \"\"  #@param {type:\"string\"}\n",
    "\n",
    "#@markdown ### Augmentation Conditions\n",
    "#@markdown Select which augmentation conditions to evaluate\n",
    "EVAL_BASELINE = True  #@param {type:\"boolean\"}\n",
    "EVAL_COLOR_JITTER = False  #@param {type:\"boolean\"}\n",
    "EVAL_GRAYSCALE = False  #@param {type:\"boolean\"}\n",
    "EVAL_GAUSSIAN_NOISE = False  #@param {type:\"boolean\"}\n",
    "EVAL_HEAVY_GEOMETRIC = False  #@param {type:\"boolean\"}\n",
    "EVAL_COMBINED_AGGRESSIVE = True  #@param {type:\"boolean\"}\n",
    "EVAL_MACENKO = False  #@param {type:\"boolean\"}\n",
    "EVAL_HED = False  #@param {type:\"boolean\"}\n",
    "\n",
    "#@markdown ### Evaluation Settings\n",
    "N_NEIGHBORS = 5  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 32  #@param {type:\"integer\"}\n",
    "\n",
    "# Process configuration\n",
    "MODEL_ID = CUSTOM_MODEL_ID if MODEL_CHOICE == \"custom\" else MODEL_CHOICE\n",
    "\n",
    "AUGMENTATIONS_TO_EVAL = []\n",
    "if EVAL_BASELINE: AUGMENTATIONS_TO_EVAL.append(\"baseline\")\n",
    "if EVAL_COLOR_JITTER: AUGMENTATIONS_TO_EVAL.append(\"color_jitter\")\n",
    "if EVAL_GRAYSCALE: AUGMENTATIONS_TO_EVAL.append(\"grayscale\")\n",
    "if EVAL_GAUSSIAN_NOISE: AUGMENTATIONS_TO_EVAL.append(\"gaussian_noise\")\n",
    "if EVAL_HEAVY_GEOMETRIC: AUGMENTATIONS_TO_EVAL.append(\"heavy_geometric\")\n",
    "if EVAL_COMBINED_AGGRESSIVE: AUGMENTATIONS_TO_EVAL.append(\"combined_aggressive\")\n",
    "if EVAL_MACENKO: AUGMENTATIONS_TO_EVAL.append(\"macenko_normalization\")\n",
    "if EVAL_HED: AUGMENTATIONS_TO_EVAL.append(\"hed_stain_augmentation\")\n",
    "\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_ID}\")\n",
    "print(f\"  Augmentations: {AUGMENTATIONS_TO_EVAL}\")\n",
    "print(f\"  HF Token: {'Set' if HF_TOKEN else 'Not set (using public models only)'}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Benchmark Dataset\n",
    "\n",
    "Downloads patches from HuggingFace Hub (~2-5 GB depending on augmentations selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Dataset repository\n",
    "DATASET_REPO = \"dellacorte/PANDA-PLUS-Bench\"\n",
    "\n",
    "print(\"Loading PANDA-PLUS-Bench dataset...\")\n",
    "print(\"(First run will download ~2-5 GB, subsequent runs use cache)\\n\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = load_dataset(DATASET_REPO, token=HF_TOKEN if HF_TOKEN else None)\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  Available splits: {list(dataset.keys())}\")\n",
    "    \n",
    "    # Show dataset info\n",
    "    sample_split = list(dataset.keys())[0]\n",
    "    print(f\"  Total patches in '{sample_split}': {len(dataset[sample_split])}\")\n",
    "    print(f\"  Features: {dataset[sample_split].features}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading dataset: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check that the dataset exists at the specified repo\")\n",
    "    print(\"  2. If private, ensure HF_TOKEN is set correctly\")\n",
    "    raise\n",
    "\n",
    "# Load paper results for comparison\n",
    "print(\"\\nLoading published benchmark results...\")\n",
    "try:\n",
    "    # Load from GitHub repo\n",
    "    import urllib.request\n",
    "    paper_results_url = \"https://raw.githubusercontent.com/dellacortelab/PANDA-PLUS-Bench/main/paper_results.json\"\n",
    "    with urllib.request.urlopen(paper_results_url) as response:\n",
    "        PAPER_RESULTS = json.loads(response.read().decode())\n",
    "    print(f\"✓ Loaded results for {len(PAPER_RESULTS)} models from paper\")\n",
    "    print(f\"  Models: {list(PAPER_RESULTS.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Paper results not found: {e}\")\n",
    "    print(\"  Comparison will be skipped.\")\n",
    "    PAPER_RESULTS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Foundation Model\n",
    "\n",
    "Loads the selected foundation model for embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_foundation_model(model_id, hf_token=None):\n",
    "    \"\"\"\n",
    "    Load a pathology foundation model.\n",
    "    \n",
    "    Supports:\n",
    "    - Phikon / Phikon-v2 (owkin)\n",
    "    - UNI / UNI2 (MahmoodLab)\n",
    "    - Virchow / Virchow2 (paige-ai)\n",
    "    - Any timm-compatible model\n",
    "    - Any HuggingFace transformers model\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    \n",
    "    model = None\n",
    "    processor = None\n",
    "    embed_dim = None\n",
    "    \n",
    "    # Try HuggingFace transformers first\n",
    "    try:\n",
    "        model = AutoModel.from_pretrained(\n",
    "            model_id, \n",
    "            token=hf_token,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            processor = AutoImageProcessor.from_pretrained(\n",
    "                model_id, \n",
    "                token=hf_token\n",
    "            )\n",
    "        except:\n",
    "            processor = None\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        if hasattr(model.config, 'hidden_size'):\n",
    "            embed_dim = model.config.hidden_size\n",
    "        elif hasattr(model, 'embed_dim'):\n",
    "            embed_dim = model.embed_dim\n",
    "        else:\n",
    "            embed_dim = None\n",
    "            \n",
    "        print(f\"  ✓ Loaded via HuggingFace transformers\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  HuggingFace loading failed: {e}\")\n",
    "        print(f\"  Trying timm...\")\n",
    "        \n",
    "        # Try timm\n",
    "        try:\n",
    "            model = timm.create_model(model_id, pretrained=True, num_classes=0)\n",
    "            embed_dim = model.num_features\n",
    "            processor = None\n",
    "            print(f\"  ✓ Loaded via timm\")\n",
    "        except Exception as e2:\n",
    "            raise ValueError(f\"Could not load model '{model_id}' via HuggingFace or timm: {e2}\")\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    # Default transform if no processor\n",
    "    if processor is None:\n",
    "        default_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    else:\n",
    "        default_transform = None\n",
    "    \n",
    "    print(f\"  Embedding dimension: {embed_dim}\")\n",
    "    print(f\"  Device: {DEVICE}\")\n",
    "    \n",
    "    return model, processor, default_transform, embed_dim\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model, processor, transform, embed_dim = load_foundation_model(MODEL_ID, HF_TOKEN)\n",
    "print(\"\\n✓ Model ready for embedding extraction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract Embeddings\n",
    "\n",
    "Extracts embeddings for all patches in selected augmentation conditions.\n",
    "\n",
    "**Expected time:** ~5-15 minutes per augmentation on Colab GPU (T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class PatchDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for benchmark patches.\"\"\"\n",
    "    def __init__(self, hf_dataset, processor=None, transform=None):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Get image (handle different formats)\n",
    "        if 'image' in item:\n",
    "            image = item['image']\n",
    "        elif 'patch' in item:\n",
    "            image = item['patch']\n",
    "        else:\n",
    "            raise KeyError(f\"No image field found. Available: {item.keys()}\")\n",
    "        \n",
    "        # Convert to PIL if needed\n",
    "        if isinstance(image, np.ndarray):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        # Apply preprocessing\n",
    "        if self.processor is not None:\n",
    "            inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        elif self.transform is not None:\n",
    "            pixel_values = self.transform(image)\n",
    "        else:\n",
    "            raise ValueError(\"No processor or transform provided\")\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'label': item.get('label', -1),\n",
    "            'slide_id': item.get('slide_id', 'unknown')\n",
    "        }\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Extract embeddings from all patches.\n",
    "    \n",
    "    Returns:\n",
    "        embeddings: numpy array of shape (n_patches, embed_dim)\n",
    "        labels: numpy array of shape (n_patches,)\n",
    "        slide_ids: numpy array of shape (n_patches,)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    all_slide_ids = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values)\n",
    "        \n",
    "        # Extract embeddings (handle different output formats)\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        elif hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            embeddings = outputs.pooler_output\n",
    "        elif isinstance(outputs, torch.Tensor):\n",
    "            embeddings = outputs\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output format: {type(outputs)}\")\n",
    "        \n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "        all_labels.extend(batch['label'].numpy())\n",
    "        all_slide_ids.extend(batch['slide_id'])\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    labels = np.array(all_labels)\n",
    "    slide_ids = np.array(all_slide_ids)\n",
    "    \n",
    "    return embeddings, labels, slide_ids\n",
    "\n",
    "\n",
    "# Extract embeddings for each augmentation\n",
    "print(f\"\\nExtracting embeddings for {len(AUGMENTATIONS_TO_EVAL)} augmentation(s)...\\n\")\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for aug in AUGMENTATIONS_TO_EVAL:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {aug}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if aug not in dataset:\n",
    "        print(f\"  ⚠ Augmentation '{aug}' not found in dataset. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    patch_dataset = PatchDataset(\n",
    "        dataset[aug], \n",
    "        processor=processor, \n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        patch_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"  Patches: {len(patch_dataset)}\")\n",
    "    print(f\"  Batches: {len(dataloader)}\")\n",
    "    \n",
    "    embeddings, labels, slide_ids = extract_embeddings(model, dataloader, DEVICE)\n",
    "    \n",
    "    all_results[aug] = {\n",
    "        'embeddings': embeddings,\n",
    "        'labels': labels,\n",
    "        'slide_ids': slide_ids\n",
    "    }\n",
    "    \n",
    "    print(f\"  ✓ Embeddings shape: {embeddings.shape}\")\n",
    "    print(f\"  ✓ Unique slides: {len(np.unique(slide_ids))}\")\n",
    "    print(f\"  ✓ Unique labels: {np.unique(labels)}\")\n",
    "\n",
    "print(f\"\\n\\n✓ Embedding extraction complete!\")\n",
    "print(f\"  Processed {len(all_results)} augmentation(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Compute Benchmark Metrics\n",
    "\n",
    "Computes the standard PANDA-PLUS-Bench metrics:\n",
    "- **Within-slide accuracy**: Classification using patches from same slide\n",
    "- **Cross-slide accuracy**: Leave-one-slide-out classification\n",
    "- **Accuracy gap**: Within - Cross (higher = more collapse)\n",
    "- **Silhouette scores**: Cluster quality by class and by slide\n",
    "- **Confusion attribution**: Entropy of kNN label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def compute_within_cross_accuracy(embeddings, labels, slide_ids, n_neighbors=5):\n",
    "    \"\"\"Compute within-slide and cross-slide classification accuracy.\"\"\"\n",
    "    unique_slides = np.unique(slide_ids)\n",
    "    within_scores = []\n",
    "    cross_scores = []\n",
    "    \n",
    "    # Within-slide accuracy\n",
    "    for slide in unique_slides:\n",
    "        slide_mask = slide_ids == slide\n",
    "        slide_embeddings = embeddings[slide_mask]\n",
    "        slide_labels = labels[slide_mask]\n",
    "        \n",
    "        if len(slide_embeddings) < n_neighbors + 1:\n",
    "            continue\n",
    "        \n",
    "        n_test = max(1, len(slide_embeddings) // 5)\n",
    "        indices = np.random.permutation(len(slide_embeddings))\n",
    "        train_idx, test_idx = indices[:-n_test], indices[-n_test:]\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=min(n_neighbors, len(train_idx)-1))\n",
    "        knn.fit(slide_embeddings[train_idx], slide_labels[train_idx])\n",
    "        within_scores.append(knn.score(slide_embeddings[test_idx], slide_labels[test_idx]))\n",
    "    \n",
    "    # Cross-slide accuracy\n",
    "    logo = LeaveOneGroupOut()\n",
    "    for train_idx, test_idx in logo.split(embeddings, labels, slide_ids):\n",
    "        X_train, X_test = embeddings[train_idx], embeddings[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "        knn.fit(X_train, y_train)\n",
    "        cross_scores.append(knn.score(X_test, y_test))\n",
    "    \n",
    "    return np.mean(within_scores), np.mean(cross_scores)\n",
    "\n",
    "# Compute metrics for each augmentation\n",
    "print(\"\\nComputing metrics...\\n\")\n",
    "metrics = {}\n",
    "\n",
    "for aug, results in all_results.items():\n",
    "    print(f\"Computing metrics for {aug}...\")\n",
    "    \n",
    "    within_acc, cross_acc = compute_within_cross_accuracy(\n",
    "        results['embeddings'],\n",
    "        results['labels'],\n",
    "        results['slide_ids'],\n",
    "        n_neighbors=N_NEIGHBORS\n",
    "    )\n",
    "    \n",
    "    metrics[aug] = {\n",
    "        'within_accuracy': within_acc,\n",
    "        'cross_accuracy': cross_acc,\n",
    "        'accuracy_gap': within_acc - cross_acc\n",
    "    }\n",
    "    \n",
    "    print(f\"  Within-slide accuracy: {within_acc:.3f}\")\n",
    "    print(f\"  Cross-slide accuracy: {cross_acc:.3f}\")\n",
    "    print(f\"  Accuracy gap: {within_acc - cross_acc:.3f}\\n\")\n",
    "\n",
    "print(\"✓ Metrics computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Results\n",
    "\n",
    "Display your model's results in a summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create results dataframe\n",
    "results_data = []\n",
    "for aug, m in metrics.items():\n",
    "    results_data.append({\n",
    "        'Augmentation': aug,\n",
    "        'Within-Slide Acc': f\"{m['within_accuracy']:.3f}\",\n",
    "        'Cross-Slide Acc': f\"{m['cross_accuracy']:.3f}\",\n",
    "        'Accuracy Gap': f\"{m['accuracy_gap']:.3f}\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Results for {MODEL_ID}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
